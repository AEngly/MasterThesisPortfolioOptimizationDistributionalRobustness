{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d56d16",
   "metadata": {},
   "source": [
    "# Construction of Dataset for Evaluation on S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f81445",
   "metadata": {},
   "source": [
    "This notebook contains two sections:\n",
    "1. Scraping the constituents and the relevant changes over the last T years\n",
    "2. Create aggregate dataset\n",
    "3. Modification of tickers (some are wrong)\n",
    "4. API request with 'AlphaVantage' to get obtain data. Notice that an AlphaVantage API key is required.\n",
    "5. Download SP500 index data with yfinance\n",
    "6. Save relevant data to access it later from the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887e03d",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b7c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import date structuring\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "# Import required modules\n",
    "import requests\n",
    "\n",
    "# Import parser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import .csv-handler\n",
    "import csv\n",
    "\n",
    "# Import data structure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get data from Yahoo Finance\n",
    "import yfinance as yf\n",
    "\n",
    "# Import download bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Import Path to automate saving of data \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194ae0f",
   "metadata": {},
   "source": [
    "## 1. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76c711",
   "metadata": {},
   "source": [
    "We cannot make any quality guarantees, but for the purpose of performance testing it should suffice. The following Wikipedia page provides an extensive overview of the constituents and the historical changes to the index maintained by S&P500.\n",
    "\n",
    "Link: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de535d",
   "metadata": {},
   "source": [
    "We start by specifying the range of interest. This is limited by the amount of historical data that can be obtained through AlphaVantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "223bba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "startingYear = 1999\n",
    "startingMonth = 1\n",
    "startingDay = 1\n",
    "\n",
    "startingDate = date(startingYear, startingMonth, startingDay)\n",
    "endDate = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642ca62",
   "metadata": {},
   "source": [
    "Then we specify the URL's to obtain the constituents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "750ae5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main URL\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    " \n",
    "# Get URL\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04bc4b",
   "metadata": {},
   "source": [
    "Once the content has been fetched with a HTTP request, we need to parse it. For this, we will use the BeautifulSoup library. This process is usually referred to as the actual scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee497c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e1a2",
   "metadata": {},
   "source": [
    "Then we wish to scrabe the table elements from the DOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b2b7c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate number of tables\n",
    "len(soup.find_all('table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f476eef",
   "metadata": {},
   "source": [
    "We can also scrape the tables individually by locating the right 'id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "104d0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableConstituents = soup.find(id='constituents')\n",
    "tableChanges = soup.find(id='changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7a74a",
   "metadata": {},
   "source": [
    "Then we specify the column names in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "043ba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNamesConstituents = [elem.text.strip() for elem in tableConstituents.find_all('th')]\n",
    "columnNamesChanges = [elem.text.strip() for elem in tableChanges.find_all('th')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d7f6b",
   "metadata": {},
   "source": [
    "The changes column does however have a different structure. Therefore, we remove 'Added' and 'Removed' and keep the order in mind later. In addition, we move 'Reason' to be the last element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ecfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNamesChanges = ['Date', 'AddedTicker', 'AddedSecurity', 'RemovedTicker', 'RemovedSecurity', 'Reason']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574336f",
   "metadata": {},
   "source": [
    "Next task is to parse the rows iteratively. From inspecting the webpage, it is seen that each row has the tag \\<tr> and all the entries in the table are found with the tag \\<td>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55165303",
   "metadata": {},
   "source": [
    "### 1.1 Scrape Rows from Constituents Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d2a3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in tableConstituents.find_all('tr'):\n",
    "    rowList = [elem.text.strip() for elem in row.find_all('td')]\n",
    "    if rowList: rows.append(rowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d07eb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contituents = pd.DataFrame(rows, columns=columnNamesConstituents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "904d18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to date format\n",
    "contituents[\"Date added\"] = pd.to_datetime(contituents[\"Date added\"], format=\"%Y-%m-%d\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681edb5",
   "metadata": {},
   "source": [
    "Some of the dates are missing. This is however not important as they are currently included in the S\\&P500 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4cc2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>D</td>\n",
       "      <td>Dominion Energy</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Electric Utilities</td>\n",
       "      <td>Richmond, Virginia</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000715957</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>FCX</td>\n",
       "      <td>Freeport-McMoRan</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Copper</td>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000831259</td>\n",
       "      <td>1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>HUM</td>\n",
       "      <td>Humana</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Managed Health Care</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000049071</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>ROK</td>\n",
       "      <td>Rockwell Automation</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Electrical Components &amp; Equipment</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001024478</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>SRE</td>\n",
       "      <td>Sempra Energy</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Multi-Utilities</td>\n",
       "      <td>San Diego, California</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001032208</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>TROW</td>\n",
       "      <td>T. Rowe Price</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Asset Management &amp; Custody Banks</td>\n",
       "      <td>Baltimore, Maryland</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001113169</td>\n",
       "      <td>1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>TXN</td>\n",
       "      <td>Texas Instruments</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000097476</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>USB</td>\n",
       "      <td>U.S. Bank</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Diversified Banks</td>\n",
       "      <td>Minneapolis, Minnesota</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000036104</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>WM</td>\n",
       "      <td>Waste Management</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Environmental &amp; Facilities Services</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000823768</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>WY</td>\n",
       "      <td>Weyerhaeuser</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>Timber REITs</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000106535</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>WHR</td>\n",
       "      <td>Whirlpool Corporation</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Household Appliances</td>\n",
       "      <td>Benton Harbor, Michigan</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0000106640</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol               Security             GICS Sector  \\\n",
       "156      D        Dominion Energy               Utilities   \n",
       "211    FCX       Freeport-McMoRan               Materials   \n",
       "243    HUM                 Humana             Health Care   \n",
       "404    ROK    Rockwell Automation             Industrials   \n",
       "415    SRE          Sempra Energy               Utilities   \n",
       "435   TROW          T. Rowe Price              Financials   \n",
       "445    TXN      Texas Instruments  Information Technology   \n",
       "457    USB              U.S. Bank              Financials   \n",
       "482     WM       Waste Management             Industrials   \n",
       "490     WY           Weyerhaeuser             Real Estate   \n",
       "491    WHR  Whirlpool Corporation  Consumer Discretionary   \n",
       "\n",
       "                       GICS Sub-Industry    Headquarters Location Date added  \\\n",
       "156                   Electric Utilities       Richmond, Virginia        NaT   \n",
       "211                               Copper         Phoenix, Arizona        NaT   \n",
       "243                  Managed Health Care     Louisville, Kentucky        NaT   \n",
       "404    Electrical Components & Equipment     Milwaukee, Wisconsin        NaT   \n",
       "415                      Multi-Utilities    San Diego, California        NaT   \n",
       "435     Asset Management & Custody Banks      Baltimore, Maryland        NaT   \n",
       "445                       Semiconductors            Dallas, Texas        NaT   \n",
       "457                    Diversified Banks   Minneapolis, Minnesota        NaT   \n",
       "482  Environmental & Facilities Services           Houston, Texas        NaT   \n",
       "490                         Timber REITs      Seattle, Washington        NaT   \n",
       "491                 Household Appliances  Benton Harbor, Michigan        NaT   \n",
       "\n",
       "            CIK Founded  \n",
       "156  0000715957    1983  \n",
       "211  0000831259    1912  \n",
       "243  0000049071    1961  \n",
       "404  0001024478    1903  \n",
       "415  0001032208    1998  \n",
       "435  0001113169    1937  \n",
       "445  0000097476    1930  \n",
       "457  0000036104    1968  \n",
       "482  0000823768    1968  \n",
       "490  0000106535    1900  \n",
       "491  0000106640    1911  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then locate all the errors\n",
    "def is_nat(x):\n",
    "    return pd.isnull(x) and isinstance(x, pd._libs.tslibs.nattype.NaTType)\n",
    "\n",
    "contituents[contituents[\"Date added\"].apply(is_nat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf79be8",
   "metadata": {},
   "source": [
    "### 1.2 Scrape Rows from Changes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79741ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in tableChanges.find_all('tr'):\n",
    "    rowList = [elem.text.strip() for elem in row.find_all('td')]\n",
    "    if rowList: rows.append(rowList)\n",
    "        \n",
    "# Create Pandas dataframe\n",
    "changes = pd.DataFrame(rows, columns=columnNamesChanges)\n",
    "\n",
    "# Convert the date column to date format\n",
    "changes[\"Date\"] = pd.to_datetime(changes[\"Date\"], format=\"%B %d, %Y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a3030",
   "metadata": {},
   "source": [
    "## 2. Historical Constituents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d63e5",
   "metadata": {},
   "source": [
    "We start by specifying the current constituents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004273ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and add constituents today\n",
    "tickers = contituents['Symbol'].to_list()\n",
    "allRows = {'Date': [date.today().strftime('%Y-%m-%d')], 'Tickers': [','.join(tickers)], 'N': [len(tickers)]}\n",
    "setTickers = set(tickers)\n",
    "allTickers = set(tickers)\n",
    "\n",
    "# Modify tickers according to changes\n",
    "for DATE in np.flip(np.arange(startingDate, endDate, timedelta(days=1)).astype(datetime)):\n",
    "    \n",
    "    # Start by adding the date\n",
    "    formattedDate = DATE.strftime(\"%Y-%m-%d\")\n",
    "    allRows['Date'].append(formattedDate)\n",
    "    \n",
    "    # The filter the changes\n",
    "    match = changes[changes['Date'] == formattedDate]\n",
    "    \n",
    "    # If match, then carry out the following procedure\n",
    "    if len(match) >= 1:\n",
    "        \n",
    "        # If AddedTicker contains a ticker, then remove it (as we traverse backwards in time)\n",
    "        addedTicker = list(match['AddedTicker'].values)\n",
    "        addedTicker = set([ticker for ticker in addedTicker if ticker != ''])\n",
    "        setTickers = setTickers.difference(addedTicker)\n",
    "        \n",
    "        # If RemovedTicker contains a ticker, then add it (as we traverse backwards in time).\n",
    "        # In addition, add the ticker to the allTickers.\n",
    "        removedTicker = list(match['RemovedTicker'].values)\n",
    "        removedTicker = set([ticker for ticker in removedTicker if ticker != ''])\n",
    "        setTickers = setTickers.union(removedTicker)\n",
    "        allTickers = allTickers.union(removedTicker)\n",
    "        \n",
    "    # Add to list\n",
    "    allRows['Tickers'].append(','.join(list(setTickers)))\n",
    "    \n",
    "    # Add to list\n",
    "    allRows['N'].append(len(setTickers))\n",
    "        \n",
    "# Aggregate the data\n",
    "allTickers = list(allTickers)\n",
    "historicalConstituents = pd.DataFrame.from_dict(allRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff14fb",
   "metadata": {},
   "source": [
    "## 3. Ticker Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6689c1",
   "metadata": {},
   "source": [
    "It appears that Wikipedia does not have updates on all names changes. While it appears that some of the tickers have been listed under new ticker names, and thus can be obtained by fetching the updated ticker, they are removed as revising them manually requires too much time. This is of course a drawback with regards to the data quality. However, they are listed in a dictionary allowing any user to manually change the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96dcd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that handles all the missing matches in the AlphaVantage calls (daily)\n",
    "dailyNameChanges = {\"ADS\": \"BFH\",  # Name change\n",
    "               \"HFC\": \"EXCLUDED\",  # Does not exist in database\n",
    "               \"NYX\": \"ACQUIRED\",  # Acquired by ICE Exchange\n",
    "               \"SMS\": \"ACQUIRED\",  # Acquired by Siemens\n",
    "               \"FBHS\": \"REMOVED\",  # Removed\n",
    "               \"GR\": \"ACQUIRED\",   # Acquired\n",
    "               \"HPH\": \"BANKRUPCY\", # Backrupcy\n",
    "               \"AV\": \"PRIVATE\",    # Taken private \n",
    "               \"TRB\": \"PRIVATE\",   # Taken private \n",
    "               \"ESV\": \"UNKNOWN\",   # Unknown reason\n",
    "               \"SBL\": \"ACQUIRED\",  # Acquired by Motorola\n",
    "               \"ACE\": \"EXR\",       # Replaced due to acquisition\n",
    "               \"WIN\": \"UNKNOWN\",   # Just not there\n",
    "               \"CBE\": \"UNKNOWN\",   \n",
    "               \"FNM\": \"UNKNOWN\",\n",
    "               \"QTRN\": \"UNKNOWN\",\n",
    "               \"FRE\": \"UNKNOWN\",\n",
    "               \"LEH\": \"UNKNOWN\",\n",
    "               \"NOVL\": \"UNKNOWN\",\n",
    "               \"BS\": \"UNKNOWN\",\n",
    "               \"GLK\": \"UNKNOWN\",\n",
    "               \"TYC\": \"UNKNOWN\",\n",
    "               \"WFR\": \"UNKNOWN\",\n",
    "               \"MEE\": \"UNKNOWN\",\n",
    "               \"DJ\": \"UNKNOWN\",\n",
    "               \"KSE\": \"UNKNOWN\",\n",
    "               \"CFC\": \"UNKNOWN\",\n",
    "               \"SLR\": \"UNKNOWN\",\n",
    "               \"RX\": \"UNKNOWN\",\n",
    "               \"CEPH\": \"UNKNOWN\",\n",
    "               \"FRC\": \"UNKNOWN\",\n",
    "               \"JCP\": \"UNKNOWN\",\n",
    "               \"TLAB\": \"UNKNOWN\",\n",
    "               \"ABK\": \"UNKNOWN\",\n",
    "               \"PTV\": \"UNKNOWN\",\n",
    "               \"EK\": \"UNKNOWN\",\n",
    "               \"ABS\": \"UNKNOWN\",\n",
    "               \"DF\": \"UNKNOWN\",\n",
    "               \"CCE\": \"UNKNOWN\",\n",
    "               \"SGP\": \"UNKNOWN\",\n",
    "               \"LDW\": \"UNKNOWN\",\n",
    "               \"GENZ\": \"UNKNOWN\",\n",
    "               \"MOLX\": \"UNKNOWN\",\n",
    "               \"BMC\": \"UNKNOWN\",\n",
    "               \"TE\": \"UNKNOWN\"\n",
    "              }\n",
    "\n",
    "# Define a dictionary that handles all the missing matches in the AlphaVantage calls (weekly)\n",
    "weeklyNameChanges =  {\"JCP\": \"JCP\", \n",
    "                   \"CFC\": \"CFC\", \n",
    "                   \"CEPH\": \"CEPH\",\n",
    "                   \"NYX\": \"NYX\",\n",
    "                   \"KSE\": \"KSE\",\n",
    "                   \"NOVL\": \"NOVL\",\n",
    "                   \"SBL\": \"SBL\",\n",
    "                   \"ABS\": \"ABS\",\n",
    "                   \"FNM\": \"FNM\",\n",
    "                   \"FRC\": \"FRC\",\n",
    "                   \"ABK\": \"ABK\",\n",
    "                   \"BS\": \"BS\",\n",
    "                   \"GLK\": \"GLK\",\n",
    "                   \"TLAB\": \"TLAB\",\n",
    "                   \"HPH\": \"HPH\",\n",
    "                   \"GR\": \"GR\",\n",
    "                   \"PTV\": \"PTV\",\n",
    "                   \"LEH\": \"LEH\",\n",
    "                   \"QTRN\": \"QTRN\",\n",
    "                   \"FBHS\": \"FBHS\",\n",
    "                   \"TYC\": \"TYC\",\n",
    "                   \"AV\": \"AV\",\n",
    "                   \"MOLX\": \"MOLX\",\n",
    "                   \"ESV\": \"ESV\",\n",
    "                   \"SGP\": \"SGP\",\n",
    "                   \"MEE\": \"MEE\",\n",
    "                   \"DJ\": \"DJ\",\n",
    "                   \"SLR\": \"SLR\",\n",
    "                   \"LDW\": \"LDW\",\n",
    "                   \"HFC\": \"HFC\",\n",
    "                   \"EK\": \"EK\",\n",
    "                   \"GENZ\": \"GENZ\",\n",
    "                   \"FRE\": \"FRE\",\n",
    "                   \"ACE\": \"ACE\",\n",
    "                   \"BMC\": \"BMC\",\n",
    "                   \"CBE\": \"CBE\",\n",
    "                   \"WFR\": \"WFR\",\n",
    "                   \"RX\": \"RX\",\n",
    "                   \"TRB\": \"TRB\",\n",
    "                   \"CCE\": \"CCE\",\n",
    "                   \"WIN\": \"WIN\",\n",
    "                   \"TE\": \"TE\",\n",
    "                   \"SMS\": \"SMS\",\n",
    "                   \"DF\": \"DF\"\n",
    "                  }\n",
    "\n",
    "# Then we filter the tickers\n",
    "filteredTickersDaily = [ticker for ticker in allTickers if ticker not in dailyNameChanges]\n",
    "filteredTickersWeekly = [ticker for ticker in allTickers if ticker not in weeklyNameChanges]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682c80a",
   "metadata": {},
   "source": [
    "## 4. Call *AlphaVantage* to Fetch Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2cec",
   "metadata": {},
   "source": [
    "In this section, we construct a AlphaVantage-class to handle the communication with the external API. Afterwards, the class is used to construct the data sets containing the historical prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25a86282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaVantageFinance:\n",
    "    \n",
    "    def __init__(self, API_KEY=\"0Y981SYNSMU72WEL\"):\n",
    "        \n",
    "        # Get API-key here: https://www.alphavantage.co/support/#api-key\n",
    "        self.API_KEY = API_KEY\n",
    "        \n",
    "        # Placeholder for latest download\n",
    "        self.latestDownload = None\n",
    "        self.latestErrors = []\n",
    "        \n",
    "        # Error in downloads\n",
    "        self.errors = 0\n",
    "\n",
    "    def downloadPrices(self, symbol=\"IBM\", function=\"DAILY_ADJUSTED\", outputSize=\"full\", sampleTypes=[\"5. adjusted close\"]):\n",
    "\n",
    "        # Description:\n",
    "        # Purpose: Function is used to download historical prices\n",
    "        data = {}\n",
    "\n",
    "        # Specify call on specific ticker\n",
    "        try:\n",
    "            if function == \"DAILY_ADJUSTED\":\n",
    "                incompleteURL = 'https://www.alphavantage.co/query?function=TIME_SERIES_{}&outputsize={}&symbol={}&apikey={}'\n",
    "                URL = incompleteURL.format(function, outputSize, symbol, self.API_KEY)\n",
    "                r = requests.get(URL)\n",
    "                data = r.json()['Time Series (Daily)']\n",
    "\n",
    "            elif function == \"WEEKLY_ADJUSTED\":\n",
    "                incompleteURL = 'https://www.alphavantage.co/query?function=TIME_SERIES_{}&symbol={}&apikey={}'\n",
    "                URL = incompleteURL.format(function, symbol, self.API_KEY)\n",
    "                r = requests.get(URL)\n",
    "                data = r.json()['Weekly Adjusted Time Series']\n",
    "\n",
    "            elif function == \"MONTHLY_ADJUSTED\":\n",
    "                incompleteURL = 'https://www.alphavantage.co/query?function=TIME_SERIES_{}&symbol={}&apikey={}'\n",
    "                URL = incompleteURL.format(function, symbol, self.API_KEY)\n",
    "                r = requests.get(URL)\n",
    "                data = r.json()['Time Series (Monthly)']\n",
    "\n",
    "            else:\n",
    "                print(\"The variable 'dataType' did not match. See documentation.\")\n",
    "        \n",
    "        except KeyError:\n",
    "            self.errors += 1\n",
    "            self.latestErrors.append(symbol)\n",
    "            #print(\"Error {} occured for the following ticker: {}\".format(self.errors, symbol))\n",
    "            \n",
    "\n",
    "        # Total samples\n",
    "        N = len(data)\n",
    "        M = len(sampleTypes)\n",
    "\n",
    "        # Allocate memory\n",
    "        levels = np.zeros((N,M))\n",
    "        dates = np.zeros(N, dtype='datetime64[s]')\n",
    "\n",
    "        # Destructure JSON object\n",
    "        for idx, key in enumerate(data):\n",
    "            for jdx, sampleType in enumerate(sampleTypes):\n",
    "                levels[idx, jdx] = data[key][sampleType]\n",
    "\n",
    "            # Save date\n",
    "            timeComponents = [int(string) for string in key.split(\"-\")]\n",
    "            year = timeComponents[0]\n",
    "            month = timeComponents[1]\n",
    "            day = timeComponents[2]\n",
    "            dates[idx] = date(year, month, day)\n",
    "            \n",
    "        # Then we can build a Pandas dataframe\n",
    "        df = pd.DataFrame()\n",
    "        df['Dates'] = dates\n",
    "        \n",
    "        # Save all the prices\n",
    "        for jdx, sampleType in enumerate(sampleTypes):\n",
    "            df[sampleType] = levels[:, jdx]\n",
    "        \n",
    "        # Set latest download\n",
    "        self.latestDownload = df\n",
    "\n",
    "        # Return the data\n",
    "        return df\n",
    "    \n",
    "    def searchTicker(self, keyword=\"microsoft\"):\n",
    "        \n",
    "        incompleteURL = 'https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={}&apikey={}'\n",
    "        URL = incompleteURL.format(keyword, self.API_KEY)\n",
    "        r = requests.get(URL)\n",
    "        matches = r.json()['bestMatches']\n",
    "        df = pd.DataFrame.from_dict(matches)\n",
    "        return df\n",
    "    \n",
    "    def downloadManyPrices(self, symbols=[\"IBM\", \"MSTF\"], function=\"DAILY_ADJUSTED\", outputSize=\"full\", sampleType=\"5. adjusted close\"):\n",
    " \n",
    "        # Description:\n",
    "        # Purpose: Function is used to download historical prices for a list of tickers\n",
    "        \n",
    "        # Setup for download bar\n",
    "        with tqdm(total=len(symbols)) as pbar:\n",
    "            \n",
    "            # Download data on first ticker\n",
    "            data = self.downloadPrices(symbol=symbols[0], function=function, sampleTypes=[sampleType])\n",
    "            data = data.rename(columns={sampleType: symbols[0]})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Download data on all the other tickers\n",
    "            for symbol in symbols[1:]:\n",
    "\n",
    "                # Download symbol/ticker data\n",
    "                df = self.downloadPrices(symbol=symbol, function=function, sampleTypes=[sampleType])\n",
    "                df = df.rename(columns={sampleType: symbol})\n",
    "\n",
    "                # Join the dataframe with the exisiting one\n",
    "                data = pd.merge(data, df, on='Dates', how=\"outer\")\n",
    "                \n",
    "                # Increment download bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "        # Cast date column into datetime\n",
    "        data[\"Dates\"] = pd.to_datetime(data[\"Dates\"], format=\"%Y-%m-%d\", errors='coerce')\n",
    "            \n",
    "        # Return the data\n",
    "        return data\n",
    "    \n",
    "    def returnPricesInterval(self, startDate = '1900-1-1', endDate = datetime.today(), symbols=[\"IBM\"], function=\"DAILY_ADJUSTED\", sampleType=\"5. adjusted close\", redownload=False):\n",
    "        \n",
    "        # Start download if not already done\n",
    "        if redownload or self.latestDownload is None:\n",
    "            self.latestDownload = self.downloadManyPrices(symbols=symbols, function=function, sampleType=sampleType)\n",
    "            \n",
    "        # Then return the appropriate range\n",
    "        data = self.latestDownload[self.latestDownload['Dates'] >= startDate]\n",
    "        data = data[data['Dates'] <= endDate]\n",
    "        \n",
    "        # Sort data\n",
    "        data = data.sort_values(by=\"Dates\")\n",
    "        \n",
    "        # Then return the data\n",
    "        return data\n",
    "\n",
    "    def downloadFundamentals(self, symbol=\"IBM\", function=\"BALANCE_SHEET\", reports=\"annualReports\", filters=[\"hej\"]):\n",
    "\n",
    "        # Description\n",
    "        # Purpose: Function is used to download historical balance sheets\n",
    "\n",
    "        # Specify call on specific ticker\n",
    "        if function == \"BALANCE_SHEET\":\n",
    "            incompleteURL = 'https://www.alphavantage.co/query?function={}&symbol={}&apikey={}'\n",
    "            URL = incompleteURL.format(function, symbol, API_KEY)\n",
    "            r = requests.get(URL)\n",
    "            data = r.json()[reports]\n",
    "        else:\n",
    "            print(\"The variable 'dataType' did not match. See documentation.\")\n",
    "\n",
    "        # Total reports and filter length\n",
    "        N = len(data)\n",
    "        F = len(filters)\n",
    "\n",
    "        # Allocate memory\n",
    "        np.zeros((N+1,F))\n",
    "\n",
    "        # Test\n",
    "        df = pd.DataFrame([data[0]])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def modifyTickers(self, tickers, pattern=r'\\b\\w+\\.\\w+\\b', replaceWith=\"-\", replaceWhich=\".\"):\n",
    "        \n",
    "        # Join all list elements\n",
    "        text = \" \".join(tickers)\n",
    "        \n",
    "        # Search for strings with dot\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        # Modify the matches\n",
    "        modifiedMatches = [string.replace(replaceWhich, replaceWith) for string in matches]\n",
    "        \n",
    "        # Remove the matches and add the modified ones\n",
    "        for match in matches:\n",
    "            tickers.remove(match)\n",
    "            tickers.append(modifiedMatches.pop())\n",
    "        \n",
    "        # Return all matches\n",
    "        return tickers, matches, modifiedMatches\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f80b9",
   "metadata": {},
   "source": [
    "In the following, we download the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eae553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class\n",
    "financeAPI = AlphaVantageFinance()\n",
    "\n",
    "# Modify the tickers to fit the right format for the API (replace \".\" with \"-\")\n",
    "modifiedTickersDaily, matches, modifiedMatches = financeAPI.modifyTickers(filteredTickersDaily, pattern=r'\\b\\w+\\.\\w+\\b', replaceWith=\"-\", replaceWhich=\".\")\n",
    "modifiedTickersWeekly, matches, modifiedMatches = financeAPI.modifyTickers(filteredTickersWeekly, pattern=r'\\b\\w+\\.\\w+\\b', replaceWith=\"-\", replaceWhich=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa51e9",
   "metadata": {},
   "source": [
    "We use the following call to fetch the data in a specified time interval. The following cell takes approximately 20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                     | 16/753 [00:13<09:08,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "# Select from relevant interval (if none, then all available data is returned)\n",
    "aggregateDailySP500 = financeAPI.returnPricesInterval(symbols=modifiedTickersDaily, function=\"DAILY_ADJUSTED\", sampleType=\"5. adjusted close\", redownload=True)\n",
    "aggregateWeeklySP500 = financeAPI.returnPricesInterval(symbols=modifiedTickersWeekly, function=\"WEEKLY_ADJUSTED\", sampleType=\"5. adjusted close\", redownload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89155ca7",
   "metadata": {},
   "source": [
    "## 5. Download Historical Data for SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382418dd",
   "metadata": {},
   "source": [
    "In this section, we download the index data for S&P500 (tciker: SPX) via Yahoo Finance. This is because Alpha Vantage does not support indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1337e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download daily data\n",
    "dailySP500Index = yf.download(\n",
    "    tickers = [\"^GSPC\"],\n",
    "    start = min(aggregateDailySP500['Dates']),\n",
    "    interval = \"1d\"\n",
    ")['Adj Close']\n",
    "\n",
    "dailySP500Index = pd.DataFrame({'Dates': dailySP500Index.index.values,'SPX-INDEX': dailySP500Index.values})\n",
    "\n",
    "# Download weekly data\n",
    "weeklySP500Index = yf.download(\n",
    "    tickers = [\"^GSPC\"],\n",
    "    start = min(aggregateWeeklySP500['Dates']),\n",
    "    interval = \"1wk\"\n",
    ")['Adj Close']\n",
    "\n",
    "weeklySP500 = pd.DataFrame({'Dates': weeklySP500Index.index.values,'SPX-INDEX': weeklySP500Index.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75258df",
   "metadata": {},
   "source": [
    "Then we can join the index values of SPX with all the assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDailySP500 = pd.merge(dailySP500Index, aggregateDailySP500, on='Dates', how=\"right\")\n",
    "completeWeeklySP500 = pd.merge(dailySP500Index, aggregateWeeklySP500, on='Dates', how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeWeeklySP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769021",
   "metadata": {},
   "source": [
    "## 6. Save Constituents and Historical Data to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f39b69",
   "metadata": {},
   "source": [
    "In this section, we save the constructed dataframes as .csv-files to ease reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac16d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct filepaths\n",
    "filepath1 = Path('./SP500/HistoricalConstituents.csv')  \n",
    "filepath2 = Path('./SP500/DailyHistoricalPrices.csv')  \n",
    "filepath3 = Path('./SP500/WeeklyHistoricalPrices.csv')\n",
    "\n",
    "# Create necessary directories\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct data sets\n",
    "historicalConstituents.to_csv(filepath1, index=False)\n",
    "completeDailySP500.to_csv(filepath2, index=False)\n",
    "completeWeeklySP500.to_csv(filepath3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d56d16",
   "metadata": {},
   "source": [
    "# Construction of Datasets for Evaluation on S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f81445",
   "metadata": {},
   "source": [
    "This notebook contains 6 sections:\n",
    "1. Scraping the constituents and the relevant changes over the last T years\n",
    "2. Create aggregate dataset\n",
    "3. Modification of tickers (some are wrong)\n",
    "4. API request with 'AlphaVantage' to get obtain data. Notice that an AlphaVantage API key is required.\n",
    "5. Download SP500 index data with yfinance\n",
    "6. Save relevant data to access it later from the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887e03d",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b7c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import date structuring\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "# Import required modules\n",
    "import requests\n",
    "\n",
    "# Import parser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import .csv-handler\n",
    "import csv\n",
    "\n",
    "# Import data structure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get data from Yahoo Finance\n",
    "import yfinance as yf\n",
    "\n",
    "# Import download bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import regex\n",
    "import re\n",
    "\n",
    "# Import Path to automate saving of data \n",
    "from pathlib import Path\n",
    "\n",
    "# Import Alpha Vantage library\n",
    "from EITP.Data.DataFetcher import AlphaVantageFinance\n",
    "from EITP.Data.DataLoader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194ae0f",
   "metadata": {},
   "source": [
    "## 1. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76c711",
   "metadata": {},
   "source": [
    "We cannot make any quality guarantees, but for the purpose of performance testing it should suffice. The following Wikipedia page provides an extensive overview of the constituents and the historical changes to the index maintained by S&P500.\n",
    "\n",
    "Link: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de535d",
   "metadata": {},
   "source": [
    "We start by specifying the range of interest. This is limited by the amount of historical data that can be obtained through AlphaVantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223bba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "startingYear = 1999\n",
    "startingMonth = 1\n",
    "startingDay = 1\n",
    "\n",
    "startingDate = date(startingYear, startingMonth, startingDay)\n",
    "endDate = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642ca62",
   "metadata": {},
   "source": [
    "Then we specify the URL's to obtain the constituents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ae5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main URL\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    " \n",
    "# Get URL\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04bc4b",
   "metadata": {},
   "source": [
    "Once the content has been fetched with a HTTP request, we need to parse it. For this, we will use the BeautifulSoup library. This process is usually referred to as the actual scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee497c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape webpage\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e1a2",
   "metadata": {},
   "source": [
    "Then we wish to scrabe the table elements from the DOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate number of tables\n",
    "len(soup.find_all('table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f476eef",
   "metadata": {},
   "source": [
    "We can also scrape the tables individually by locating the right 'id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableConstituents = soup.find(id='constituents')\n",
    "tableChanges = soup.find(id='changes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7a74a",
   "metadata": {},
   "source": [
    "Then we specify the column names in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNamesConstituents = [elem.text.strip() for elem in tableConstituents.find_all('th')]\n",
    "columnNamesChanges = [elem.text.strip() for elem in tableChanges.find_all('th')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d7f6b",
   "metadata": {},
   "source": [
    "The changes column does however have a different structure. Therefore, we remove 'Added' and 'Removed' and keep the order in mind later. In addition, we move 'Reason' to be the last element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNamesChanges = ['Date', 'AddedTicker', 'AddedSecurity', 'RemovedTicker', 'RemovedSecurity', 'Reason']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574336f",
   "metadata": {},
   "source": [
    "Next task is to parse the rows iteratively. From inspecting the webpage, it is seen that each row has the tag \\<tr> and all the entries in the table are found with the tag \\<td>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55165303",
   "metadata": {},
   "source": [
    "### 1.1 Scrape Rows from Constituents Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in tableConstituents.find_all('tr'):\n",
    "    rowList = [elem.text.strip() for elem in row.find_all('td')]\n",
    "    if rowList: rows.append(rowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07eb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contituents = pd.DataFrame(rows, columns=columnNamesConstituents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to date format\n",
    "contituents[\"Date added\"] = pd.to_datetime(contituents[\"Date added\"], format=\"%Y-%m-%d\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681edb5",
   "metadata": {},
   "source": [
    "Some of the dates are missing. This is however not important as they are currently included in the S\\&P500 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then locate all the errors\n",
    "def is_nat(x):\n",
    "    return pd.isnull(x) and isinstance(x, pd._libs.tslibs.nattype.NaTType)\n",
    "\n",
    "contituents[contituents[\"Date added\"].apply(is_nat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf79be8",
   "metadata": {},
   "source": [
    "### 1.2 Scrape Rows from Changes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79741ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in tableChanges.find_all('tr'):\n",
    "    rowList = [elem.text.strip() for elem in row.find_all('td')]\n",
    "    if rowList: rows.append(rowList)\n",
    "        \n",
    "# Create Pandas dataframe\n",
    "changes = pd.DataFrame(rows, columns=columnNamesChanges)\n",
    "\n",
    "# Convert the date column to date format\n",
    "changes[\"Date\"] = pd.to_datetime(changes[\"Date\"], format=\"%B %d, %Y\", errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a3030",
   "metadata": {},
   "source": [
    "## 2. Historical Constituents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d63e5",
   "metadata": {},
   "source": [
    "We start by specifying the current constituents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004273ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and add constituents today\n",
    "tickers = contituents['Symbol'].to_list()\n",
    "allRows = {'Date': [date.today().strftime('%Y-%m-%d')], 'Tickers': [','.join(tickers)], 'N': [len(tickers)]}\n",
    "setTickers = set(tickers)\n",
    "allTickers = set(tickers)\n",
    "\n",
    "# Modify tickers according to changes\n",
    "for DATE in np.flip(np.arange(startingDate, endDate, timedelta(days=1)).astype(datetime)):\n",
    "    \n",
    "    # Start by adding the date\n",
    "    formattedDate = DATE.strftime(\"%Y-%m-%d\")\n",
    "    allRows['Date'].append(formattedDate)\n",
    "    \n",
    "    # The filter the changes\n",
    "    match = changes[changes['Date'] == formattedDate]\n",
    "    \n",
    "    # If match, then carry out the following procedure\n",
    "    if len(match) >= 1:\n",
    "        \n",
    "        # If AddedTicker contains a ticker, then remove it (as we traverse backwards in time)\n",
    "        addedTicker = list(match['AddedTicker'].values)\n",
    "        addedTicker = set([ticker for ticker in addedTicker if ticker != ''])\n",
    "        setTickers = setTickers.difference(addedTicker)\n",
    "        \n",
    "        # If RemovedTicker contains a ticker, then add it (as we traverse backwards in time).\n",
    "        # In addition, add the ticker to the allTickers.\n",
    "        removedTicker = list(match['RemovedTicker'].values)\n",
    "        removedTicker = set([ticker for ticker in removedTicker if ticker != ''])\n",
    "        setTickers = setTickers.union(removedTicker)\n",
    "        allTickers = allTickers.union(removedTicker)\n",
    "        \n",
    "    # Add to list\n",
    "    allRows['Tickers'].append(','.join(list(setTickers)))\n",
    "    \n",
    "    # Add to list\n",
    "    allRows['N'].append(len(setTickers))\n",
    "        \n",
    "# Aggregate the data\n",
    "allTickers = list(allTickers)\n",
    "historicalConstituents = pd.DataFrame.from_dict(allRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff14fb",
   "metadata": {},
   "source": [
    "## 3. Ticker Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6689c1",
   "metadata": {},
   "source": [
    "It appears that Wikipedia does not have updates on all names changes. While it appears that some of the tickers have been listed under new ticker names, and thus can be obtained by fetching the updated ticker, they are removed as revising them manually requires too much time. This is of course a drawback with regards to the data quality. However, they are listed in a dictionary allowing any user to manually change the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that handles all the missing matches in the AlphaVantage calls (daily)\n",
    "dailyNameChanges = {\"ADS\": \"BFH\",  # Name change\n",
    "               \"HFC\": \"EXCLUDED\",  # Does not exist in database\n",
    "               \"NYX\": \"ACQUIRED\",  # Acquired by ICE Exchange\n",
    "               \"SMS\": \"ACQUIRED\",  # Acquired by Siemens\n",
    "               \"FBHS\": \"REMOVED\",  # Removed\n",
    "               \"GR\": \"ACQUIRED\",   # Acquired\n",
    "               \"HPH\": \"BANKRUPCY\", # Backrupcy\n",
    "               \"AV\": \"PRIVATE\",    # Taken private \n",
    "               \"TRB\": \"PRIVATE\",   # Taken private \n",
    "               \"ESV\": \"UNKNOWN\",   # Unknown reason\n",
    "               \"SBL\": \"ACQUIRED\",  # Acquired by Motorola\n",
    "               \"ACE\": \"EXR\",       # Replaced due to acquisition\n",
    "               \"WIN\": \"UNKNOWN\",   # Just not there\n",
    "               \"CBE\": \"UNKNOWN\",   \n",
    "               \"FNM\": \"UNKNOWN\",\n",
    "               \"QTRN\": \"UNKNOWN\",\n",
    "               \"FRE\": \"UNKNOWN\",\n",
    "               \"LEH\": \"UNKNOWN\",\n",
    "               \"NOVL\": \"UNKNOWN\",\n",
    "               \"BS\": \"UNKNOWN\",\n",
    "               \"GLK\": \"UNKNOWN\",\n",
    "               \"TYC\": \"UNKNOWN\",\n",
    "               \"WFR\": \"UNKNOWN\",\n",
    "               \"MEE\": \"UNKNOWN\",\n",
    "               \"DJ\": \"UNKNOWN\",\n",
    "               \"KSE\": \"UNKNOWN\",\n",
    "               \"CFC\": \"UNKNOWN\",\n",
    "               \"SLR\": \"UNKNOWN\",\n",
    "               \"RX\": \"UNKNOWN\",\n",
    "               \"CEPH\": \"UNKNOWN\",\n",
    "               \"FRC\": \"UNKNOWN\",\n",
    "               \"JCP\": \"UNKNOWN\",\n",
    "               \"TLAB\": \"UNKNOWN\",\n",
    "               \"ABK\": \"UNKNOWN\",\n",
    "               \"PTV\": \"UNKNOWN\",\n",
    "               \"EK\": \"UNKNOWN\",\n",
    "               \"ABS\": \"UNKNOWN\",\n",
    "               \"DF\": \"UNKNOWN\",\n",
    "               \"CCE\": \"UNKNOWN\",\n",
    "               \"SGP\": \"UNKNOWN\",\n",
    "               \"LDW\": \"UNKNOWN\",\n",
    "               \"GENZ\": \"UNKNOWN\",\n",
    "               \"MOLX\": \"UNKNOWN\",\n",
    "               \"BMC\": \"UNKNOWN\",\n",
    "               \"TE\": \"UNKNOWN\"\n",
    "              }\n",
    "\n",
    "# Define a dictionary that handles all the missing matches in the AlphaVantage calls (weekly)\n",
    "weeklyNameChanges =  {\"JCP\": \"JCP\", \n",
    "                   \"CFC\": \"CFC\", \n",
    "                   \"CEPH\": \"CEPH\",\n",
    "                   \"NYX\": \"NYX\",\n",
    "                   \"KSE\": \"KSE\",\n",
    "                   \"NOVL\": \"NOVL\",\n",
    "                   \"SBL\": \"SBL\",\n",
    "                   \"ABS\": \"ABS\",\n",
    "                   \"FNM\": \"FNM\",\n",
    "                   \"FRC\": \"FRC\",\n",
    "                   \"ABK\": \"ABK\",\n",
    "                   \"BS\": \"BS\",\n",
    "                   \"GLK\": \"GLK\",\n",
    "                   \"TLAB\": \"TLAB\",\n",
    "                   \"HPH\": \"HPH\",\n",
    "                   \"GR\": \"GR\",\n",
    "                   \"PTV\": \"PTV\",\n",
    "                   \"LEH\": \"LEH\",\n",
    "                   \"QTRN\": \"QTRN\",\n",
    "                   \"FBHS\": \"FBHS\",\n",
    "                   \"TYC\": \"TYC\",\n",
    "                   \"AV\": \"AV\",\n",
    "                   \"MOLX\": \"MOLX\",\n",
    "                   \"ESV\": \"ESV\",\n",
    "                   \"SGP\": \"SGP\",\n",
    "                   \"MEE\": \"MEE\",\n",
    "                   \"DJ\": \"DJ\",\n",
    "                   \"SLR\": \"SLR\",\n",
    "                   \"LDW\": \"LDW\",\n",
    "                   \"HFC\": \"HFC\",\n",
    "                   \"EK\": \"EK\",\n",
    "                   \"GENZ\": \"GENZ\",\n",
    "                   \"FRE\": \"FRE\",\n",
    "                   \"ACE\": \"ACE\",\n",
    "                   \"BMC\": \"BMC\",\n",
    "                   \"CBE\": \"CBE\",\n",
    "                   \"WFR\": \"WFR\",\n",
    "                   \"RX\": \"RX\",\n",
    "                   \"TRB\": \"TRB\",\n",
    "                   \"CCE\": \"CCE\",\n",
    "                   \"WIN\": \"WIN\",\n",
    "                   \"TE\": \"TE\",\n",
    "                   \"SMS\": \"SMS\",\n",
    "                   \"DF\": \"DF\"\n",
    "                  }\n",
    "\n",
    "# Then we filter the tickers\n",
    "filteredTickersDaily = [ticker for ticker in allTickers if ticker not in dailyNameChanges]\n",
    "filteredTickersWeekly = [ticker for ticker in allTickers if ticker not in weeklyNameChanges]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682c80a",
   "metadata": {},
   "source": [
    "## 4. Call *AlphaVantage* to Fetch Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2cec",
   "metadata": {},
   "source": [
    "In this section, we use our custom AlphaVantage class to handle the communication with the external API. Afterwards, the class is used to construct the data sets containing the historical prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class\n",
    "dailyFinanceAPI = AlphaVantageFinance()\n",
    "weeklyFinanceAPI = AlphaVantageFinance()\n",
    "\n",
    "# Modify the tickers to fit the right format for the API (replace \".\" with \"-\")\n",
    "modifiedTickersDaily, matchesWeekly, modifiedMatchesDaily = dailyFinanceAPI.modifyTickers(filteredTickersDaily, pattern=r'\\b\\w+\\.\\w+\\b', replaceWith=\"-\", replaceWhich=\".\")\n",
    "modifiedTickersWeekly, matchesWeekly, modifiedMatchesWeekly = weeklyFinanceAPI.modifyTickers(filteredTickersWeekly, pattern=r'\\b\\w+\\.\\w+\\b', replaceWith=\"-\", replaceWhich=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa51e9",
   "metadata": {},
   "source": [
    "We use the following call to fetch the data in a specified time interval. The following cell takes approximately 20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from relevant interval (if none, then all available data is returned)\n",
    "aggregateDailySP500 = dailyFinanceAPI.returnPricesInterval(symbols=modifiedTickersDaily, function=\"DAILY_ADJUSTED\", sampleType=\"5. adjusted close\", redownload=False)\n",
    "aggregateWeeklySP500 = weeklyFinanceAPI.returnPricesInterval(symbols=modifiedTickersWeekly, function=\"WEEKLY_ADJUSTED\", sampleType=\"5. adjusted close\", redownload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89155ca7",
   "metadata": {},
   "source": [
    "## 5. Download Historical Data for SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382418dd",
   "metadata": {},
   "source": [
    "In this section, we download the index data for S&P500 (ticker: SPX) via Yahoo Finance. This is because Alpha Vantage does not support indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1337e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download daily data\n",
    "dailySP500Index = yf.download(\n",
    "    tickers = [\"^GSPC\"],\n",
    "    start = min(aggregateDailySP500['Dates']),\n",
    "    interval = \"1d\"\n",
    ")['Adj Close']\n",
    "\n",
    "dailySP500Index = pd.DataFrame({'Dates': dailySP500Index.index.values,'SPX-INDEX': dailySP500Index.values})\n",
    "\n",
    "# Download weekly data\n",
    "weeklySP500Index = yf.download(\n",
    "    tickers = [\"^GSPC\"],\n",
    "    start = min(aggregateWeeklySP500['Dates']),\n",
    "    interval = \"1wk\"\n",
    ")['Adj Close']\n",
    "\n",
    "weeklySP500 = pd.DataFrame({'Dates': weeklySP500Index.index.values,'SPX-INDEX': weeklySP500Index.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75258df",
   "metadata": {},
   "source": [
    "Then we can join the index values of SPX with all the assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDailySP500 = pd.merge(dailySP500Index, aggregateDailySP500, on='Dates', how=\"right\")\n",
    "completeWeeklySP500 = pd.merge(dailySP500Index, aggregateWeeklySP500, on='Dates', how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769021",
   "metadata": {},
   "source": [
    "## 6. Save Constituents, GICS Categories and Historical Data to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f39b69",
   "metadata": {},
   "source": [
    "In this section, we save the constructed dataframes as .csv-files to ease reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac16d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct filepaths\n",
    "filepath1 = Path('./Data/SP500/HistoricalConstituents.csv')  \n",
    "filepath2 = Path('./Data/SP500/DailyHistoricalPrices.csv')  \n",
    "filepath3 = Path('./Data/SP500/WeeklyHistoricalPrices.csv')\n",
    "filepath4 = Path('./Data/SP500/ConstituentInformation.csv')\n",
    "\n",
    "# Create necessary directories\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct data sets\n",
    "historicalConstituents.to_csv(filepath1, index=False)\n",
    "completeDailySP500.to_csv(filepath2, index=False)\n",
    "completeWeeklySP500.to_csv(filepath3, index=False)\n",
    "contituents.to_csv(filepath4, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee47e91",
   "metadata": {},
   "source": [
    "We test reading the constituents and create a dictionary that can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath4 = Path('./Data/SP500/ConstituentInformation.csv')\n",
    "test = pd.read_csv(filepath4)\n",
    "industryMap = dict()\n",
    "sectors = test['GICS Sector'].values\n",
    "subIndustries = test['GICS Sector'].values\n",
    "for idx, ticker in enumerate(test['Symbol'].values):\n",
    "    industryMap[ticker] = {'Sector': sectors[idx],'Sub-Industry': subIndustries[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d3639",
   "metadata": {},
   "source": [
    "## 7. Creating a filtered data set with Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddc9ad",
   "metadata": {},
   "source": [
    "We initialize the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b42fd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = DataLoader(path='./Data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13b5d5",
   "metadata": {},
   "source": [
    "Then we can save SP500RawFull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c450a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from raw file\n",
    "dailySP500RawFull = DL.SP500(freq=\"daily\", intersect=False, startDate=\"1999-01-01\", endDate=dt.datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "dailyRawTickers = np.array(dailyStockData.columns[2:], dtype=str)\n",
    "\n",
    "# Load data from raw file\n",
    "weeklySP500RawFull = DL.SP500(freq=\"weekly\", intersect=False, startDate=\"1999-01-01\", endDate=dt.datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "weeklySP500RawFull = weeklySP500RawFull.loc[:, dailySP500RawFull.columns]\n",
    "weeklyRawTickers = dailyRawTickers\n",
    "\n",
    "# Construct filepaths\n",
    "filepath1 = Path('./Data/SP500/dailySP500RawFull.csv')\n",
    "filepath2 = Path('./Data/SP500/weeklySP500RawFull.csv')  \n",
    "\n",
    "# Create necessary directories\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "filepath2.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct data sets\n",
    "dailySP500RawFull.to_csv(filepath1, index=False)\n",
    "weeklySP500RawFull.to_csv(filepath2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc689d8",
   "metadata": {},
   "source": [
    "Then we can save SP500RawIntersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22189e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from raw file\n",
    "dailySP500RawIntersect = DL.SP500(freq=\"daily\", intersect=True, startDate=\"1999-01-01\", endDate=dt.datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "dailyIntersectTickers = np.array(dailySP500RawIntersect.columns[2:], dtype=str)\n",
    "\n",
    "# Load data from raw file\n",
    "weeklySP500RawIntersect = DL.SP500(freq=\"weekly\", intersect=True, startDate=\"1999-01-01\", endDate=dt.datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "weeklySP500RawIntersect = weeklySP500RawIntersect.loc[:, dailySP500RawIntersect.columns]\n",
    "weeklyIntersectTickers = dailyIntersectTickers\n",
    "\n",
    "# Construct filepaths\n",
    "filepath1 = Path('./Data/SP500/dailySP500RawIntersect.csv')\n",
    "filepath2 = Path('./Data/SP500/weeklySP500RawIntersect.csv')  \n",
    "\n",
    "# Create necessary directories\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "filepath2.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct data sets\n",
    "dailySP500RawIntersect.to_csv(filepath1, index=False)\n",
    "weeklySP500RawIntersect.to_csv(filepath2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99832f",
   "metadata": {},
   "source": [
    "## Exclude GICS sub-industries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd73b8",
   "metadata": {},
   "source": [
    "Then we try to exclude GICS sectors that we do not want to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50331b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "industryMap = DL.filterIndustries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e6f9e",
   "metadata": {},
   "source": [
    "Suppose we want to exclude the following categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aca91a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = ['Aerospace & Defense', \n",
    "             'Oil & Gas Equipment & Services'\n",
    "             'Casinos & Gaming',\n",
    "             'Integrated Oil & Gas',\n",
    "             'Oil & Gas Exploration & Production',\n",
    "             'Oil & Gas Refining & Marketing',\n",
    "             'Tobacco',\n",
    "             'Brewers',\n",
    "             'Distillers & Vintners',\n",
    "             'Oil & Gas Storage & Transportation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d016db",
   "metadata": {},
   "source": [
    "Then we filter our tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "14ddda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with raw\n",
    "filteredTickersRaw = DL.filterTickers(dailyRawTickers, blacklist)\n",
    "\n",
    "# .. then intersect\n",
    "filteredTickersIntersect = DL.filterTickers(dailyIntersectTickers, blacklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966295cf",
   "metadata": {},
   "source": [
    "Then we modifiy the stock data to only include these. Due to inconsistency in the downloaded prices, the weekly dataset has an additional column. It is removed by considering the columns from the daily dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69bc4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating it on raw\n",
    "selector = list(dailySP500RawFull.columns[:2]) + filteredTickersRaw\n",
    "dailySP500FilteredFull = dailySP500RawFull.loc[:,selector]\n",
    "weeklySP500FilteredFull = weeklySP500RawFull.loc[:,selector]\n",
    "\n",
    "# .. then create it on filtered\n",
    "selector = list(dailySP500RawIntersect.columns[:2]) + filteredTickersIntersect\n",
    "dailySP500FilteredIntersect = dailySP500RawIntersect.loc[:,selector]\n",
    "weeklySP500FilteredIntersect = weeklySP500RawIntersect.loc[:,selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752f6c8",
   "metadata": {},
   "source": [
    "Now the stock data holds all those assets not blacklisted. These can be fed directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e96592b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct filepaths\n",
    "filepath1 = Path('./Data/SP500/dailySP500FilteredFull.csv')\n",
    "filepath2 = Path('./Data/SP500/weeklySP500FilteredFull.csv')  \n",
    "filepath3 = Path('./Data/SP500/dailySP500FilteredIntersect.csv')\n",
    "filepath4 = Path('./Data/SP500/weeklySP500FilteredIntersect.csv') \n",
    "\n",
    "# Create necessary directories\n",
    "filepath1.parent.mkdir(parents=True, exist_ok=True)\n",
    "filepath2.parent.mkdir(parents=True, exist_ok=True)\n",
    "filepath3.parent.mkdir(parents=True, exist_ok=True)\n",
    "filepath4.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct data sets\n",
    "dailySP500FilteredFull.to_csv(filepath1, index=False)\n",
    "weeklySP500FilteredFull.to_csv(filepath2, index=False)\n",
    "dailySP500FilteredIntersect.to_csv(filepath3, index=False)\n",
    "weeklySP500FilteredIntersect.to_csv(filepath4, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
